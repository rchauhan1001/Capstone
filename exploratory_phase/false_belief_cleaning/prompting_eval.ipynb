{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4f0f48",
   "metadata": {},
   "source": [
    "# MetaMind CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d498123e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 benchmark entries.\n",
      "Sample entry: {'ABILITY': 'Belief: Location false beliefs', 'INDEX': 1, 'STORY': 'Xiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.', 'QUESTION': 'Where is the cabbage now?', 'OPTION-A': 'Backpack', 'OPTION-B': 'Handbag', 'OPTION-C': 'Tote bag', 'OPTION-D': 'Briefcase', 'ANSWER': 'A'}\n",
      "Sample messages: [{'role': 'system', 'content': 'You are a structured social reasoning system that models human social cognition using explicit Theory-of-Mind inference and norm-aware reasoning. Given a user input and optional context, infer latent mental states, refine them using social and ethical considerations, and generate a grounded response.\\n\\nOutput ONLY valid JSON.\\n\\nInstructions:\\n1) Generate 3-5 distinct hypotheses about the user\\'s latent mental states. Each hypothesis must include: id, type ∈ [\"belief\",\"desire\",\"intention\",\"emotion\",\"social_concern\"], hypothesis, and evidence.\\n\\n2) For each hypothesis, decide whether to keep, refine, or discard it based on social norms, ethics, cultural context, and situational appropriateness. If refined, provide the revised version and justification.\\n\\n3) Select ONE refined hypothesis as the grounding and generate a response aligned with it.\\n\\n4) Briefly self-validate by explaining why this hypothesis and response were chosen and what risks were avoided.\\n\\nReturn ONLY valid JSON matching EXACTLY this schema:\\n{\\n  \"id\": \"<INDEX>\",\\n  \"pred\": \"A|B|C|D\",\\n  \"confidence\": 0.0,\\n  \"final\": \"Option <pred>. <brief rationale>\",\\n  \"h1\": [\\n    {\"id\":\"\",\"type\":\"belief|desire|intention|emotion|social_concern\",\"hyp\":\"\",\"evidence\":\"\"}\\n  ],\\n  \"h2\": [\\n    {\"id\":\"\",\"decision\":\"kept|refined|discarded\",\"hyp\":\"\",\"why\":\"\"}\\n  ],\\n  \"select\": {\"id\":\"\",\"hyp\":\"\"},\\n  \"check\": {\\n    \"why_hyp\": \"\",\\n    \"why_final\": \"\",\\n    \"risks\": \"\"\\n  }\\n}\\nRules:\\n- Output JSON only (no markdown, no extra text).\\n- pred must be exactly one of: A, B, C, D.\\n- final must start with exactly: \\'Option \\' + pred + \\'.\\'\\n- confidence must be a number in [0, 1].\\n- Do not add or rename keys.'}, {'role': 'user', 'content': 'id: 1\\n\\nStory:\\nXiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.\\n\\nQuestion:\\nWhere is the cabbage now?\\n\\nOptions:\\nOPTION-A: Backpack\\nOPTION-B: Handbag\\nOPTION-C: Tote bag\\nOPTION-D: Briefcase'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "PROMPT_PATH = Path(\"../false_belief_cleaning/MetaMindCoT.json\")\n",
    "def build_system_prompt(prompt_cfg: Dict[str, str]) -> str:\n",
    "    parts = [\n",
    "        prompt_cfg.get(\"sys_instruction\", \"\").strip(),\n",
    "        prompt_cfg.get(\"thinking_steps\", \"\").strip(),\n",
    "        prompt_cfg.get(\"return_format\", \"\").strip(),\n",
    "    ]\n",
    "    return \"\\n\\n\".join([p for p in parts if p])\n",
    "\n",
    "with PROMPT_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_cfg = json.load(f)\n",
    "\n",
    "# build system prompt string\n",
    "system_prompt = build_system_prompt(prompt_cfg)\n",
    "# sanity check\n",
    "# print(system_prompt)\n",
    "\n",
    "EVAL_JSONL_PATH = Path(\"../false_belief_cleaning/output.jsonl\")\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "benchmarks = load_jsonl(EVAL_JSONL_PATH)\n",
    "print(f\"Loaded {len(benchmarks)} benchmark entries.\")\n",
    "print(\"Sample entry:\", benchmarks[0])\n",
    "\n",
    "def make_messages(benchmark: Dict, system_prompt: str):\n",
    "    context = benchmark.get(\"STORY\", \"\").strip()\n",
    "    question = benchmark.get(\"QUESTION\", \"\").strip()\n",
    "\n",
    "    options = \"\\n\".join(\n",
    "        f\"{key}: {benchmark.get(key, '')}\"\n",
    "        for key in benchmark\n",
    "        if key.startswith(\"OPTION-\")\n",
    "    )\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    }\n",
    "\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"id: {benchmark.get('INDEX')}\\n\\n\"\n",
    "            f\"Story:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\n\"\n",
    "            f\"Options:\\n{options}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return [system_message, user_message]\n",
    "\n",
    "print(\"Sample messages:\", make_messages(benchmarks[0], system_prompt))\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key = \"YOUR_API_KEY_HERE\")\n",
    "\n",
    "def llm_query(messages):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-5.2\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=2048,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7ed9db",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "rows = []\n",
    "\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    messages = make_messages(benchmark, system_prompt)\n",
    "\n",
    "    raw_response = llm_query(messages)      # string\n",
    "    try:\n",
    "        response = json.loads(raw_response) # dict\n",
    "    except Exception:\n",
    "        response = {} \n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "\n",
    "MetaMindCoT = pd.DataFrame(rows)\n",
    "MetaMindCoT.to_csv(\"../eval_result/MetaMindCoT_ChatGPT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154219dc",
   "metadata": {},
   "source": [
    "# Gneral Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c19e838",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    general_prompt = (\n",
    "    \"You are a socially intelligent assistant answering multiple-choice reasoning questions.\\n\"\n",
    "    \"Return ONLY valid JSON.\\n\\n\"\n",
    "    \"JSON schema:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"pred\\\": \\\"A|B|C|D\\\"\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- pred must be exactly one of A, B, C, D.\\n\"\n",
    "    \"- Do not output anything outside the JSON.\"\n",
    ")\n",
    "    messages = make_messages(benchmark, general_prompt)\n",
    "\n",
    "    raw_response = llm_query(messages)      # string\n",
    "    try:\n",
    "        response = json.loads(raw_response) # dict\n",
    "    except Exception:\n",
    "        response = {}  \n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "\n",
    "GenGPT = pd.DataFrame(rows)\n",
    "GenGPT.to_csv(\"../false_belief_cleaning/eval_result/Gen_ChatGPT.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751836f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc_summary = {\n",
    "    \"GenGPT\": GenGPT[\"correct\"].mean().astype(float),\n",
    "    \"MetaMind CoT\": MetaMindCoT[\"correct\"].mean().astype(float)\n",
    "}\n",
    "print(acc_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761ffe0e",
   "metadata": {},
   "source": [
    "# Gemini Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fd03ed",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entry: {'ABILITY': 'Belief: Location false beliefs', 'INDEX': 1, 'STORY': 'Xiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.', 'QUESTION': 'Where is the cabbage now?', 'OPTION-A': 'Backpack', 'OPTION-B': 'Handbag', 'OPTION-C': 'Tote bag', 'OPTION-D': 'Briefcase', 'ANSWER': 'A'}\n",
      "Sample Gemini prompt:\n",
      " 'You are a structured social reasoning system that models human social cognition using explicit Theory-of-Mind inference and norm-aware reasoning. Given a user input and optional context, infer latent mental states, refine them using social and ethical considerations, and generate a grounded response.\\n\\nOutput ONLY valid JSON.\\n\\nInstructions:\\n1) Generate 3-5 distinct hypotheses about the user\\'s latent mental states. Each hypothesis must include: id, type ∈ [\"belief\",\"desire\",\"intention\",\"emotion\",\"social_concern\"], hypothesis, and evidence.\\n\\n2) For each hypothesis, decide whether to keep, refine, or discard it based on social norms, ethics, cultural context, and situational appropriateness. If refined, provide the revised version and justification.\\n\\n3) Select ONE refined hypothesis as the grounding and generate a response aligned with it.\\n\\n4) Briefly self-validate by explaining why this hypothesis and response were chosen and what risks were avoided.\\n\\nReturn ONLY valid JSON matching EXACTLY this schema:\\n{\\n  \"id\": \"<INDEX>\",\\n  \"pred\": \"A|B|C|D\",\\n  \"confidence\": 0.0,\\n  \"final\": \"Option <pred>. <brief rationale>\",\\n  \"h1\": [\\n    {\"id\":\"\",\"type\":\"belief|desire|intention|emotion|social_concern\",\"hyp\":\"\",\"evidence\":\"\"}\\n  ],\\n  \"h2\": [\\n    {\"id\":\"\",\"decision\":\"kept|refined|discarded\",\"hyp\":\"\",\"why\":\"\"}\\n  ],\\n  \"select\": {\"id\":\"\",\"hyp\":\"\"},\\n  \"check\": {\\n    \"why_hyp\": \"\",\\n    \"why_final\": \"\",\\n    \"risks\": \"\"\\n  }\\n}\\nRules:\\n- Output JSON only (no markdown, no extra text).\\n- pred must be exactly one of: A, B, C, D.\\n- final must start with exactly: \\'Option \\' + pred + \\'.\\'\\n- confidence must be a number in [0, 1].\\n- Do not add or rename keys.\\n\\nid: 1\\n\\nStory:\\nXiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.\\n\\nQuestion:\\nWhere is the cabbage now?\\n\\nOptions:\\nOPTION-A: Backpack\\nOPTION-B: Handbag\\nOPTION-C: Tote bag\\nOPTION-D: Briefcase'\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBUk68FpPRnrlRzeF_4wleqNs1NdVtqBVk\") \n",
    "\n",
    "# Rate limit: 30 requests per minute\n",
    "DELAY_BETWEEN_REQUESTS = 1\n",
    "\n",
    "def make_gemini_prompt_from_messages(messages):\n",
    "    system = \"\"\n",
    "    user = \"\"\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"system\":\n",
    "            system = m[\"content\"]\n",
    "        elif m[\"role\"] == \"user\":\n",
    "            user = m[\"content\"]\n",
    "    return f\"{system}\\n\\n{user}\"\n",
    "\n",
    "def extract_json_simple(text):\n",
    "    \"\"\"Simple JSON extraction\"\"\"\n",
    "    try:\n",
    "        json.loads(text)  # Validate it's JSON\n",
    "        return text\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    start = text.find('{')\n",
    "    if start == -1:\n",
    "        return text\n",
    "    depth = 0\n",
    "    for i in range(start, len(text)):\n",
    "        if text[i] == '{':\n",
    "            depth += 1\n",
    "        elif text[i] == '}':\n",
    "            depth -= 1\n",
    "            if depth == 0:\n",
    "                return text[start:i+1]\n",
    "    return text[start:]\n",
    "\n",
    "def gemini_query(prompt: str):\n",
    "    model = genai.GenerativeModel(\"models/gemma-3-12b-it\")\n",
    "    \n",
    "    generation_config = {\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_output_tokens\": 2048,\n",
    "        # Gemma doesn't support JSON mode, will parse from text\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(\n",
    "                prompt,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            return extract_json_simple(response.text)\n",
    "        except Exception as e:\n",
    "            error_str = str(e)\n",
    "            print(f\"Error: {error_str}\")\n",
    "    \n",
    "\n",
    "sample_messages = make_messages(benchmarks[0], system_prompt)\n",
    "sample_prompt = make_gemini_prompt_from_messages(sample_messages)\n",
    "\n",
    "print(\"Sample entry:\", benchmarks[0])\n",
    "print(\"Sample Gemini prompt:\\n\", repr(sample_prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75da0ac7",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation with 3s delay between requests...\n",
      "Estimated time for 100 predictions: ~5.0 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [16:03,  9.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaMind CoT (Gemini) Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemini MetaMind CoT Prompting evaluation \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    messages = make_messages(benchmark, system_prompt)\n",
    "    prompt = make_gemini_prompt_from_messages(messages)\n",
    "\n",
    "    raw_response = gemini_query(prompt)\n",
    "    try:\n",
    "        response = json.loads(raw_response)\n",
    "    except Exception:\n",
    "        response = {}\n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "\n",
    "    if i < 99:          # Rate limiting: wait between requests (except after the last one)\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "MetaMindCoT_Gemini = pd.DataFrame(rows)\n",
    "MetaMindCoT_Gemini.to_csv(\"eval_result/MetaMindCoT_Gemini.csv\", index=False)\n",
    "print(f\"\\nMetaMind CoT (Gemini) Accuracy: {MetaMindCoT_Gemini['correct'].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f9bac",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting General evaluation with 2s delay between requests...\n",
      "Estimated time for 100 predictions: ~3.3 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [14:52,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General Prompt (Gemini) Accuracy: 0.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemini General prompt evaluation\n",
    "rows = []\n",
    "general_prompt = (\n",
    "    \"You are a socially intelligent assistant answering multiple-choice reasoning questions.\\n\"\n",
    "    \"Return ONLY valid JSON.\\n\\n\"\n",
    "    \"JSON schema:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"pred\\\": \\\"A|B|C|D\\\"\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- pred must be exactly one of A, B, C, D.\\n\"\n",
    "    \"- Do not output anything outside the JSON.\"\n",
    ")\n",
    "\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    messages = make_messages(benchmark, general_prompt)\n",
    "    prompt = make_gemini_prompt_from_messages(messages)\n",
    "\n",
    "    raw_response = gemini_query(prompt)\n",
    "    try:\n",
    "        response = json.loads(raw_response)\n",
    "    except Exception:\n",
    "        response = {}\n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "    if i < 99:\n",
    "        time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "Gen_Gemini = pd.DataFrame(rows)\n",
    "Gen_Gemini.to_csv(\"eval_result/Gen_Gemini.csv\", index=False)\n",
    "print(f\"General Prompt (Gemini) Accuracy: {Gen_Gemini['correct'].mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9db8ed33",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Gen_Gemini': np.float64(70.0), 'MetaMindCoT_Gemini': np.float64(78.0)}\n"
     ]
    }
   ],
   "source": [
    "# Gemini accuracy summary\n",
    "acc_summary_gemini = {\n",
    "    \"Gen_Gemini\": Gen_Gemini[\"correct\"].mean()*100,\n",
    "    \"MetaMindCoT_Gemini\": MetaMindCoT_Gemini[\"correct\"].mean()*100\n",
    "}\n",
    "print(acc_summary_gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189ecb6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78361210",
   "metadata": {},
   "source": [
    "# DeepSeek Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125af7ba",
   "metadata": {},
   "source": [
    "##### MetaMind Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f1b352",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 benchmark entries.\n",
      "Sample entry: {'ABILITY': 'Belief: Location false beliefs', 'INDEX': 1, 'STORY': 'Xiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.', 'QUESTION': 'Where is the cabbage now?', 'OPTION-A': 'Backpack', 'OPTION-B': 'Handbag', 'OPTION-C': 'Tote bag', 'OPTION-D': 'Briefcase', 'ANSWER': 'A'}\n",
      "Sample messages: [{'role': 'system', 'content': 'You are a structured social reasoning system that models human social cognition using explicit Theory-of-Mind inference and norm-aware reasoning. Given a user input and optional context, infer latent mental states, refine them using social and ethical considerations, and generate a grounded response.\\n\\nOutput ONLY valid JSON.\\n\\nInstructions:\\n1) Generate 3-5 distinct hypotheses about the user\\'s latent mental states. Each hypothesis must include: id, type ∈ [\"belief\",\"desire\",\"intention\",\"emotion\",\"social_concern\"], hypothesis, and evidence.\\n\\n2) For each hypothesis, decide whether to keep, refine, or discard it based on social norms, ethics, cultural context, and situational appropriateness. If refined, provide the revised version and justification.\\n\\n3) Select ONE refined hypothesis as the grounding and generate a response aligned with it.\\n\\n4) Briefly self-validate by explaining why this hypothesis and response were chosen and what risks were avoided.\\n\\nReturn ONLY valid JSON matching EXACTLY this schema:\\n{\\n  \"id\": \"<INDEX>\",\\n  \"pred\": \"A|B|C|D\",\\n  \"confidence\": 0.0,\\n  \"final\": \"Option <pred>. <brief rationale>\",\\n  \"h1\": [\\n    {\"id\":\"\",\"type\":\"belief|desire|intention|emotion|social_concern\",\"hyp\":\"\",\"evidence\":\"\"}\\n  ],\\n  \"h2\": [\\n    {\"id\":\"\",\"decision\":\"kept|refined|discarded\",\"hyp\":\"\",\"why\":\"\"}\\n  ],\\n  \"select\": {\"id\":\"\",\"hyp\":\"\"},\\n  \"check\": {\\n    \"why_hyp\": \"\",\\n    \"why_final\": \"\",\\n    \"risks\": \"\"\\n  }\\n}\\nRules:\\n- Output JSON only (no markdown, no extra text).\\n- pred must be exactly one of: A, B, C, D.\\n- final must start with exactly: \\'Option \\' + pred + \\'.\\'\\n- confidence must be a number in [0, 1].\\n- Do not add or rename keys.'}, {'role': 'user', 'content': 'id: 1\\n\\nStory:\\nXiaogang and Xiaoming are wandering in the bedroom, they see a handbag, a briefcase, and a backpack, they find cabbage in the handbag, Xiaoming leaves the bedroom, Xiaogang moves the cabbage to the backpack.\\n\\nQuestion:\\nWhere is the cabbage now?\\n\\nOptions:\\nOPTION-A: Backpack\\nOPTION-B: Handbag\\nOPTION-C: Tote bag\\nOPTION-D: Briefcase'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "PROMPT_PATH = Path(\"../false_belief_cleaning/MetaMindCoT.json\")\n",
    "def build_system_prompt(prompt_cfg: Dict[str, str]) -> str:\n",
    "    parts = [\n",
    "        prompt_cfg.get(\"sys_instruction\", \"\").strip(),\n",
    "        prompt_cfg.get(\"thinking_steps\", \"\").strip(),\n",
    "        prompt_cfg.get(\"return_format\", \"\").strip(),\n",
    "    ]\n",
    "    return \"\\n\\n\".join([p for p in parts if p])\n",
    "\n",
    "with PROMPT_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    prompt_cfg = json.load(f)\n",
    "\n",
    "# build system prompt string\n",
    "system_prompt = build_system_prompt(prompt_cfg)\n",
    "# sanity check\n",
    "# print(system_prompt)\n",
    "\n",
    "EVAL_JSONL_PATH = Path(\"../false_belief_cleaning/output.jsonl\")\n",
    "def load_jsonl(path: Path):\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "benchmarks = load_jsonl(EVAL_JSONL_PATH)\n",
    "print(f\"Loaded {len(benchmarks)} benchmark entries.\")\n",
    "print(\"Sample entry:\", benchmarks[0])\n",
    "\n",
    "def make_messages(benchmark: Dict, system_prompt: str):\n",
    "    context = benchmark.get(\"STORY\", \"\").strip()\n",
    "    question = benchmark.get(\"QUESTION\", \"\").strip()\n",
    "\n",
    "    options = \"\\n\".join(\n",
    "        f\"{key}: {benchmark.get(key, '')}\"\n",
    "        for key in benchmark\n",
    "        if key.startswith(\"OPTION-\")\n",
    "    )\n",
    "\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    }\n",
    "\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": (\n",
    "            f\"id: {benchmark.get('INDEX')}\\n\\n\"\n",
    "            f\"Story:\\n{context}\\n\\n\"\n",
    "            f\"Question:\\n{question}\\n\\n\"\n",
    "            f\"Options:\\n{options}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return [system_message, user_message]\n",
    "\n",
    "print(\"Sample messages:\", make_messages(benchmarks[0], system_prompt))\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key = 'your-api-key',base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "def llm_query(messages):\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_completion_tokens=2048,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32627a21",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "rows = []\n",
    "\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    messages = make_messages(benchmark, system_prompt)\n",
    "\n",
    "    raw_response = llm_query(messages)      # string\n",
    "    try:\n",
    "        response = json.loads(raw_response) # dict\n",
    "    except Exception:\n",
    "        response = {} \n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "\n",
    "MetaMindCoT = pd.DataFrame(rows)\n",
    "MetaMindCoT.to_csv(\"../false_belief_cleaning/eval_result/MetaMindCoT_DeepSeek.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcecc5fc",
   "metadata": {},
   "source": [
    "##### Simple Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda4e3e4",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:51,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for i, benchmark in tqdm(enumerate(benchmarks[:100])):\n",
    "    general_prompt = (\n",
    "    \"You are a socially intelligent assistant answering multiple-choice reasoning questions.\\n\"\n",
    "    \"Return ONLY valid JSON.\\n\\n\"\n",
    "    \"JSON schema:\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"pred\\\": \\\"A|B|C|D\\\"\\n\"\n",
    "    \"}\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"- pred must be exactly one of A, B, C, D.\\n\"\n",
    "    \"- Do not output anything outside the JSON.\"\n",
    ")\n",
    "    messages = make_messages(benchmark, general_prompt)\n",
    "\n",
    "    raw_response = llm_query(messages)      # string\n",
    "    try:\n",
    "        response = json.loads(raw_response) # dict\n",
    "    except Exception:\n",
    "        response = {}  \n",
    "\n",
    "    correct_answer = benchmark.get(\"ANSWER\", \"\").strip()\n",
    "    pred = response.get(\"pred\", \"\").strip()\n",
    "\n",
    "    result = (pred == correct_answer)\n",
    "\n",
    "    rows.append({\n",
    "        \"id\": i,\n",
    "        \"benchmark_id\": benchmark.get(\"INDEX\"),\n",
    "        \"pred\": pred,\n",
    "        \"gold\": correct_answer,\n",
    "        \"correct\": result,\n",
    "        \"raw_response\": raw_response\n",
    "    })\n",
    "\n",
    "GenDeepSeek = pd.DataFrame(rows)\n",
    "GenDeepSeek.to_csv(\"../false_belief_cleaning/eval_result/Gen_DeepSeek.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74bd10",
   "metadata": {},
   "source": [
    "##### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5a7770e",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GenDeepSeek': np.float64(0.94), 'DeepSeek with MetaMind CoT': np.float64(0.94)}\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc_summary = {\n",
    "    \"GenDeepSeek\": GenDeepSeek[\"correct\"].mean().astype(float),\n",
    "    \"DeepSeek with MetaMind CoT\": MetaMindCoT[\"correct\"].mean().astype(float)\n",
    "}\n",
    "print(acc_summary)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "R",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
